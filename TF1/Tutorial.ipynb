{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TUTORIAL - sample project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/object-recognition-with-deep-learning/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/\n",
    "\n",
    "https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491\n",
    "\n",
    "Book; http://neuralnetworksanddeeplearning.com/chap6.html\n",
    "\n",
    "https://www.pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "\n",
    "https://blog.paperspace.com/mask-r-cnn-tensorflow-2-0-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful ones:\n",
    "\n",
    "https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Region-Based CNN, R-CNN. Best so far is called the 'Mask R-CNN'. An upgrade to R-CNN, faster R-CNN, YOLO, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import mrcnn\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model\n",
    "from mrcnn import visualize\n",
    "from mrcnn import utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import colorsys\n",
    "import argparse\n",
    "import imutils\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Failed Attempts\n",
    "\n",
    "# # TF 1.14 implementation - Medium article (doesn't really work due to some log issue)\n",
    "# class myMaskRCNNConfig(Config):\n",
    "#     NAME = 'MaskRCNN_inference'\n",
    "#     GPU_COUNT = 1\n",
    "#     IMAGES_PER_GPU = 1\n",
    "#     NUM_CLASSES = 1+80\n",
    "\n",
    "\n",
    "# config = myMaskRCNNConfig()\n",
    "# model = modellib.MaskRCNN(mode='inference', config=config, model_dir='./')\n",
    "\n",
    "# # TF 2.0+ implementation - Does not work - too much other issues\n",
    "\n",
    "# CLASS_NAMES = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "# # same classes as before (81)\n",
    "\n",
    "# class simpleConfig(Config):\n",
    "#     NAME = 'coco_inference'\n",
    "#     GPU_COUNT = 1\n",
    "#     IMAGES_PER_GPU = 1\n",
    "\n",
    "#     NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# model = model.MaskRCNN(mode='inference', config=simpleConfig(), model_dir=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepare the model configuration parameters\n",
    "\n",
    "class SimpleConfig(Config):\n",
    "    NAME = 'coco_inference'\n",
    "    \n",
    "    # Used to calculate batch size\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "    NUM_CLASSES = 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    /Users/OAA/PycharmProjects/Mask_RCNN/mrcnn/model.py:390 call  *\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n    /Users/OAA/PycharmProjects/Mask_RCNN/mrcnn/model.py:341 log2_graph  *\n        return tf.log(x) / tf.log(2.0)\n\n    AttributeError: module 'tensorflow' has no attribute 'log'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/dj/whhvx2nj1jx6fqgjkcwy_1fc0000gn/T/ipykernel_10446/1473891438.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 2. Building the Mask RCNN Model architecture\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m model = model.MaskRCNN(mode='inference',\n\u001B[0m\u001B[1;32m      4\u001B[0m                        \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSimpleConfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                        model_dir=os.getcwd())\n",
      "\u001B[0;32m~/PycharmProjects/Mask_RCNN/mrcnn/model.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, mode, config, model_dir)\u001B[0m\n\u001B[1;32m   1835\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_dir\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1836\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_log_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1837\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1838\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1839\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mbuild\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Mask_RCNN/mrcnn/model.py\u001B[0m in \u001B[0;36mbuild\u001B[0;34m(self, mode, config)\u001B[0m\n\u001B[1;32m   2033\u001B[0m             \u001B[0;31m# Proposal classifier and BBox regressor heads\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2034\u001B[0m             \u001B[0mmrcnn_class_logits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmrcnn_class\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmrcnn_bbox\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2035\u001B[0;31m                 fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n\u001B[0m\u001B[1;32m   2036\u001B[0m                                      \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPOOL_SIZE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNUM_CLASSES\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2037\u001B[0m                                      \u001B[0mtrain_bn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTRAIN_BN\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Mask_RCNN/mrcnn/model.py\u001B[0m in \u001B[0;36mfpn_classifier_graph\u001B[0;34m(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)\u001B[0m\n\u001B[1;32m    922\u001B[0m     \u001B[0;31m# ROI Pooling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    923\u001B[0m     \u001B[0;31m# Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 924\u001B[0;31m     x = PyramidROIAlign([pool_size, pool_size],\n\u001B[0m\u001B[1;32m    925\u001B[0m                         name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\n\u001B[1;32m    926\u001B[0m     \u001B[0;31m# Two 1024 FC layers (implemented with Conv2D for consistency)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mambaforge/envs/cnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    949\u001B[0m     \u001B[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    950\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_in_functional_construction_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 951\u001B[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001B[0m\u001B[1;32m    952\u001B[0m                                                 input_list)\n\u001B[1;32m    953\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mambaforge/envs/cnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_functional_construction_call\u001B[0;34m(self, inputs, args, kwargs, input_list)\u001B[0m\n\u001B[1;32m   1088\u001B[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001B[1;32m   1089\u001B[0m         \u001B[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1090\u001B[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001B[0m\u001B[1;32m   1091\u001B[0m             inputs, input_masks, args, kwargs)\n\u001B[1;32m   1092\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mambaforge/envs/cnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_keras_tensor_symbolic_call\u001B[0;34m(self, inputs, input_masks, args, kwargs)\u001B[0m\n\u001B[1;32m    820\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mKerasTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_signature\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 822\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_infer_output_signature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    823\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    824\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_infer_output_signature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mambaforge/envs/cnn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_infer_output_signature\u001B[0;34m(self, inputs, args, kwargs, input_masks)\u001B[0m\n\u001B[1;32m    861\u001B[0m           \u001B[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    862\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 863\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    864\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    865\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mambaforge/envs/cnn/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    668\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint:disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    669\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ag_error_metadata'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 670\u001B[0;31m           \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    671\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    672\u001B[0m           \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: in user code:\n\n    /Users/OAA/PycharmProjects/Mask_RCNN/mrcnn/model.py:390 call  *\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n    /Users/OAA/PycharmProjects/Mask_RCNN/mrcnn/model.py:341 log2_graph  *\n        return tf.log(x) / tf.log(2.0)\n\n    AttributeError: module 'tensorflow' has no attribute 'log'\n"
     ]
    }
   ],
   "source": [
    "# 2. Building the Mask RCNN Model architecture\n",
    "\n",
    "model = model.MaskRCNN(mode='inference',\n",
    "                       config=SimpleConfig(),\n",
    "                       model_dir=os.getcwd())\n",
    "\n",
    "model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the Model Weights\n",
    "\n",
    "model.load_weights(filepath='mask_rcnn_coco.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Read an input image\n",
    "\n",
    "image = cv2.imread('zebra.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Reordering the color channels to RGB, rather than BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Detect objects\n",
    "\n",
    "r = model.detect(images=[image], verbose=0)\n",
    "\n",
    "r = r[0]\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize\n",
    "\n",
    "CLASS_NAMES = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# r = r[0]\n",
    "\n",
    "visualize.display_instances(image=image, \n",
    "                            boxes=r['rois'],\n",
    "                            masks=r['masks'],\n",
    "                            class_ids=r['class_ids'],\n",
    "                            class_names=CLASS_NAMES,\n",
    "                            scores=r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Set\n",
    "\n",
    "1. load_dataset - directory of images/annots\n",
    "2. load_mask - loads masks - accepts image ID (could be anything) and returns masks/class ID for object.\n",
    "3. extract_boxes - returns coordinates of each bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrcnn\n",
    "\n",
    "from mrcnn import model\n",
    "from mrcnn import visualize\n",
    "from mrcnn import utils\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN, load_image_gt, mold_image\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import colorsys\n",
    "import argparse\n",
    "import imutils\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# from tensorflow.python.util import deprecation\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "# Doesnt work\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset - KangarooDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defining a new class to load the dataset\n",
    "class KangarooDataset(utils.Dataset):\n",
    "    \n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        self.add_class(\"dataset\", 1, \"kangaroo\") # Adding class - eg. arm - 1, hands - 0, etc\n",
    "\n",
    "        img_dir = dataset_dir + '/images/'\n",
    "        ann_dir = dataset_dir + '/annots/'\n",
    "\n",
    "        for filename in os.listdir(img_dir):\n",
    "            img_id = filename[:-4]\n",
    "\n",
    "            if img_id in ['00090']: # Skip 90 cause it's bad\n",
    "                continue\n",
    "\n",
    "            if is_train and int(img_id) >= 150: # Training data only until 150\n",
    "                continue\n",
    "\n",
    "            if not is_train and int(img_id) < 150: # Test data after 150\n",
    "                continue\n",
    "\n",
    "            img_path = img_dir + filename\n",
    "            ann_path = ann_dir + img_id + '.xml'\n",
    "\n",
    "            self.add_image('dataset', image_id=img_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # Function to extract bounding boxes from an annotation file\n",
    "    # Short-cutting development, directly extracting the data with XPath queries\n",
    "    def extract_boxes(self, filename): \n",
    "        tree = ElementTree.parse(filename) # Loading/parsing the annotation\n",
    "\n",
    "        root = tree.getroot() # retrieving the root element\n",
    "\n",
    "        boxes = list()\n",
    "        for box in root.findall('.//bndbox'): # Finding min/max values that define each bounding box\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        return boxes, width, height\n",
    "\n",
    "    # Returning one or more masks for photos and classes for each mask\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # First load the annotation file:\n",
    "        \n",
    "        info = self.image_info[image_id] # Retrieving image info dict for image_id - previously defined in load_dataset\n",
    "        path = info['annotation']\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        masks = np.zeros([h, w, len(boxes)], dtype='uint8') # Marking all as 0\n",
    "        # Mask - a 2d array with same dimensions as photograph,\n",
    "        # all zeros where background, and all ones when it's the object\n",
    "\n",
    "        # Creating masks\n",
    "        class_ids = list()\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            \n",
    "            class_ids.append(self.class_names.index('kangaroo')) # Marking the bounding box areas as 1\n",
    "            \n",
    "        return masks, np.asarray(class_ids, dtype='int32') \n",
    "    \n",
    "    \n",
    "    # Loading image reference\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info(image_id)\n",
    "        return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = KangarooDataset()\n",
    "train_set.load_dataset(dataset_dir='kangaroo-master', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "print(f'Train: {len(train_set.image_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "\n",
    "test_set = KangarooDataset()\n",
    "test_set.load_dataset(dataset_dir='kangaroo-master', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "print(f'Test: {len(test_set.image_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing:\n",
    "id = 24\n",
    "\n",
    "# Load img\n",
    "image = train_set.load_image(id)\n",
    "print(image.shape)\n",
    "\n",
    "# Load mask\n",
    "mask, class_ids = train_set.load_mask(id)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image\n",
    "plt.imshow(image)\n",
    "plt.imshow(mask[:, :, 0], cmap='gray', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Model + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KangarooConfig(Config):\n",
    "    NAME = 'kangaroo_cfg'\n",
    "    \n",
    "    NUM_CLASSES = 2 #only 2 - kangaroo + BG(background)\n",
    "    \n",
    "    STEPS_PER_EPOCH = 131\n",
    "    \n",
    "config = KangarooConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "\n",
    "\n",
    "kang_model = mrcnn.model.MaskRCNN(mode='training', \n",
    "                                  model_dir='./', \n",
    "                                  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kang_model.load_weights('mask_rcnn_coco.h5',\n",
    "                        by_name=True,\n",
    "                        exclude=['mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', 'mrcnn_mask'])\n",
    "\n",
    "# excluded are the ones responsible for class classification, bounding boxes and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kang_model.train(train_set,\n",
    "                 test_set,\n",
    "                 learning_rate=config.LEARNING_RATE,\n",
    "                 epochs=5,\n",
    "                 layers='heads') # We only train the heads - output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Kangaroo_mask_rcnn.h5'\n",
    "kan_model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "Positive if IoU is greater than 0.5.\n",
    "\n",
    "We we do is to enumerate the images in dataset, making a prediction and calculating AP for the prediction. Then get a mean one across all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to making predictions\n",
    "\n",
    "class PredictionConfig(Config):\n",
    "    NAME = 'kangaroo_cfg'\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "pred_config = PredictionConfig()\n",
    "pred_model = model.MaskRCNN(mode='inference',\n",
    "                            model_dir='./',\n",
    "                            config=pred_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the weights of the final epoch \n",
    "pred_model.load_weights('mask_rcnn_kangaroo_cfg_0005.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, model, config):\n",
    "    APs = list()\n",
    "    \n",
    "    for image_id in dataset.image_ids:\n",
    "        # Load image, bounding boxes/masks for image id\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "        \n",
    "        # Scaling pixel values of loaded image\n",
    "        scaled_image = mold_image(image, config)\n",
    "        # Convert image into one sample\n",
    "        sample = np.expand_dims(scaled_image, 0)\n",
    "        \n",
    "        # Prediction\n",
    "        yhat = model.detect(sample, verbose=0)\n",
    "        r = yhat[0] # Results for the first sample\n",
    "        \n",
    "        AP, _, _, _ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "        APs.append(AP)\n",
    "        \n",
    "    mAP = np.mean(APs)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mAP = evaluate_model(train_set, pred_model, pred_config)\n",
    "print(f\"Train mAP: {train_mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mAP = evaluate_model(test_set, pred_model, pred_config)\n",
    "print(f\"Test mAP: {test_mAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making actual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on a brand new image\n",
    "\n",
    "new_img = cv2.imread('kang_1.jpeg')\n",
    "\n",
    "# General pre-processing\n",
    "scaled_img = mold_image(new_img, pred_config)\n",
    "sample = np.expand_dims(scaled_img, 0)\n",
    "\n",
    "yhat = pred_model.detect(sample, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function, taking a dataset, model, and config, creating a plot of the first 5 photos\n",
    "\n",
    "def plot_actualVSpredicted(dataset, model, config, n_images=5):\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        img = dataset.load_image(i)\n",
    "        mask, _ = dataset.load_mask(i)\n",
    "        \n",
    "        # Standard Pre-processing\n",
    "        scaled_img = mold_image(img, pred_config)\n",
    "        sample = np.expand_dims(scaled_img, 0)\n",
    "        yhat = pred_model.detect(sample, verbose=0)[0]\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        # Original\n",
    "        plt.subplot(n_images, 2, i*2+1) # Defining subplot\n",
    "        # Plot raw pixel data\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Actual\")\n",
    "        # Plot masks\n",
    "        for j in range(mask.shape[2]):\n",
    "            plt.imshow(mask[:, :, j], cmap='gray', alpha=0.5)\n",
    "\n",
    "    \n",
    "        # A second one beside the first, with the first, plot photo again and with bounding boxes in red\n",
    "        # With bounding boxes\n",
    "        plt.subplot(n_images, 2, i*2+2)\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title('Predicted')\n",
    "        \n",
    "        # Plotting each box\n",
    "        ax=plt.gca()\n",
    "        for box in yhat['rois']:\n",
    "            y1, x1, y2, x2 = box\n",
    "            width, height = x2-x1, y2-y1\n",
    "            rect = Rectangle((x1, y1), width, height, fill=False, color='blue')\n",
    "\n",
    "            # Draw the box\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actualVSpredicted(train_set, pred_model, pred_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actualVSpredicted(test_set, pred_model, pred_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}